## first try to understand why RAG

# What is RAG?

(Query)"Prompt" ---> (LLM) --> Response

- LLM is joint transformer base neural network architecture, which is trained by a huge amount of text data from social media and it has lot of parameters ;;

- one question should be came to you mind , at which place these llm store these infomation(knowledge) --> they store information in there **parameters** that why those parameters also called --> **parameteric knowledge**

- that why we say , the number of parameters if model is high means that model is good.

- quetion aries how you can access those knowledges--> by prompting --> so as user you can send a query to the llm (technically "prompt" ) --> llm take prompt and try to answer after searching this prompt each word comparision their parameter;; this how llm works

- most of time this work (sending query to llm , llm take your query and try to answer using their knowledge or parameters) good.

- there are certain situation where this particular flow does not work. (where you can't be able to generate good response from llm's parametric knowledge). eg three situations

  1.(Private data) we ask a question which depends on my person data. (during the pretraining stage of llm it couldn't see those data).

2. (recent data) ask about today's infomation (about those data which llm haven't seen during there pretraining process). # each llm has their last knowledge cutoff date.

## doubt: we can ask these todays data or like information to chat-gpt bz it has access to internet.

3.( helusination): in some question you send you query to llm and try to access parametric knowledge --> your llm give facturly incorrect information with confidence ;; bz llm works with probabilistically ;; that is why there is probability that instead of giving facturally correct information , they assume something itself and give you

- is there any way so we can solve these three problem?, and get good response from llm's parameters knowledge? --> yes --> there is one way to solve these problem (not much effeciently but yes ). --> by doing **Fine-tunning**

## now let try to understant what is **fine-tunning**

- take a pretained llm model --> and train again on a smaller - domain specific dataset

- you have llm (trained on huge datasets) which has knowledge about all the feild but we want that , that llm model should also hava our person data knowledge --> so what we do is

- we qurate a small dataset (which belongs to our domain) , and try to train that llm on it.;; after that it doesn't only have knowledge about more field also knowledge about our datasets. --> and know it can also answer a difficult answer from our field side.

## there are diff types of fine-tunning;;

1. Supervised fine-tunning; where we provied a labeled dataset to our model. eg

- (our prompt --> desired output) -> format of this dataset. (1k - 1M like datasets)

2. Contineud pretraining ;; this a **unsupervised way** to fine-tune our model , here also we provide datasets but that dataset would be unsupervised (labels will be not there)

## there is also some other fine-tunning technic

-> RLHF, LORA, KULORA

## how exactly carry out process of Fine-tunning;; (for supervised fine-tunning) --> presisly 4 steps process

1. Collect data -> A few hundred - few hundred-thousand carefully curated examples (prompt --> desired output)

2. Choose a methods -> Full-parameter FT, LoRA/QLoRA, or parameter-efficient adapters.

3. Train for a few epochs -> You keep the base weights frozen or partially frozen and update only a small subset (LoRA) or all weights (full FT(full fine-tuning)).

4. Evaluate and safety-test -> **Measure exact-match**, factually, and **hallucination rate against held-out data**; red-team for safety.

## now let try to see how FT help us to solve those three problems

1. solved ; ;which when you train your llm model with your person data , after that it has good ideal about your person questions (now your personal data are become its parametrix knowledge part)

2. for recent data ;; like you have to fine-tune again with new datasets

3. haluciation ;; we can also solve this ;; so we have to give some tricy prompts in there training steps , where for those prompt related prompt our model was haluciating.;; so just have to say these are tricy prompt and with these tricy prompt you have to just answer "I don't know". (don't create new facts)

- so in netsell if we use **Fine-tuning** then we can easily solve these three problems

## there are also some problems with fine-tuning that why using fine-tuning at every situation is not logical.

1. major problem with fine-tunning is;; training a LLM model althrough you are training it with small dataset or big ;; it is computationally expensive;; you have to pay for it

2. you fine-tunning you should have **strong technical experties**;; proper data-engineer, data-scientist.

3. we need to re-fine-tunning when we update our person-database. (for each fine-tunning we have to spend money);; if we are removing anything from our person-dataset then again we have to fine-tune our llm model so model should now have any wrong information in its parameter

## so for these such senario we don't prefer fine-tunning to solve these three problems.

## do we have any other methods to solve this;; ---> Yes --> **In context learning**

# In-Context-learning is a core capability of large-language model(LLMs) like GPT-3/4, claude, and Llama, where the model learns to solve a task purely by seeing examples is the prompt - without updating its weights.

- and these eg which we give first are called --> **few-short prompting** (so you can teach llm model by giving egs in your related field)

- Note;; Incontext-leaning is emergent property of LLM.

## An **Emergent property** is a behaviour or ability suddenly appears in a system when it reaches a certain scale or complexity -- even through it was not explicitly programmed or expected from the individual components.

- means incontext-leaning it not programmetically make in llm ;; when llm model is mode then it suddenly show these property
- eg gpt1 and gpt2 do not have this in-context-leaning feature bz they are not trained with big dataset or do not complex but gpt3 (trained with 175B parameters) it is a complex llm model so it has this feature

- then a reasearch paper came named was--> **Language Models are Few-Short Learners**

## since it is emergent property so it does not work good in very context.

## now try to understand if we improve our In-context-leaning then how we can solve those three problems:

- for now we are doing **few-short prompting** (which means we give some eg inside our prompt and give to our model and asking similar kind of questions)

- rather than few-short prompting (give egs in prompt ) if we give whole context to solve question in the prompt then .

- live we are making a chatbot for like **helping in this video like thing** so let at any point in the video if user have any query to we send that query with that part of video (basically video transcript(will work as transcript)) where that query topic was thought. --> through that parameric knowledge of LLM model is enhanceing

## so instead of just example tasks, retrieve background information, facts, documents, product manuals, etc

## inject that into the prompt to augment the model's knowledge ;; see pic 3

# and this extra context is called RAG:

## RAG is way to makes a language model (like chatgpt) smarter by giving it extra information **at the time you ask your question**;; see pic-5

- now llm use this context and its parametric knowledge to solve that query ;;

## now try to understand how RAG can solve those 3 problems and How a rag base application works

- Rag is made on two concepts

1. Information Retriever
2. Text-generation

- Rag can be divided into four steps;

1. Indexing (when you create an **external knowledge base** , so using that external knowledge base we drive this **context** which is sent to prompt);; so making process of **external knowledge base** is called **indexing**

2. Retrieval ;; it is process where we go to **external knowledge base** with user query and try to find all those chunkes which related to that user query. and drive **context** using to related chunkes

3. Augmentation ;; when you make prompt useing **retrieved context** and **your query** (so this prompt building process is called au..); augmentaion why; bz we are augmentate extra context to llm knowledge parameter

4. Generation; when this prompt is reached to llm then llm use its text-generation and given context to answer that query is called generation.

## (1. Indexing) - indexing is the process of **preparing your 'knowledge base' so that it can be efficiently searched** at query time. This steps consists of 4 sub-steps

1. Document ingestion -- you load your source knowledge into memory.

- eg
- - PDF reports, Word documents
- - Youtube transcripts, blog pages
- - GitHub repos, internal wikis
- - SQL records, scraped webpages

- for that we use diff Tools (document loaders)
- - LangChain loaders (pyPDFLoader, YouTubeLoader, WebBaseLoader, GitLoader, etc.)

2. Text Chunking - Break large documents into small, semantically meaningful **chunks**

- Why chunk?
- - LLMs have context limits (eg. 4k-32k tokens)
- - Smaller chunks are more focused = better semantic search

- Tools( for do this);
- RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter, SemanticChunker

## (3. Embedding Generation) - Convert each chunk into a a **dense Vector** (embedding) that captures its meaning.

- Why embedding?
- - Similar ideas land close together in vector space
- - Allows fast, fuzzy semantic search
    \*\* Tools
- OpenAiEmbeddings, SentenceTransformerEmbeddings, InstructorEmbeddings. etc

## (4 Storage) in a vector store -- store the vectors along with the origin chunk text + metadata in a vector database.

- Vector DB options -> local: FAISS, Chroma;; Cloud: Pinecone, Weaviate, Milvus, Qdrant

## now at the final step of indexing we have this vector store or vector db --> is our **External knowledge**

## (2. Retriever - ) is the real-time process of **finding the most relevant pieces of information** from a pre-build index (created during indexing) based on the user's question.

- step 1;; it generate embedding vector for our query (by using same embedding model)
- step 2;; search closed chunks in vector db
- step 3;; ranking (most closed first)
- step 4;; fetch most relevent chunks(text) from db which will be our --> **context**

## (3. augmentation) ;; creating prompt which have (query + context)

## (4. generation);; final rag step ;; Generation is the final step where a **Large Language Model(LLM)** uses the **user's query** and the **retrieved and augmented context** to generate a response.

## now come to those three question and try to understand how we can solve those using rag pipeline

1. private data --> solved by **external knowledge base** ;; context would be come from your person dataset
2. Recent develoments or every llm has a knowledge cutoff date --> put current data in **external knowledge base**
3. Hallucinations --> solved bz we are providing exact context in our prompt (not giving any chance to our llm model to give any diff response)

## rag is chiffer and alternative solution to Fine-tunning
