## first try to understand why RAG

# What is RAG?

(Query)"Prompt" ---> (LLM) --> Response

- LLM is joint transformer base neural network architecture, which is trained by a huge amount of text data from social media and it has lot of parameters ;;

- one question should be came to you mind , at which place these llm store these infomation(knowledge) --> they store information in there **parameters** that why those parameters also called --> **parameteric knowledge**

- that why we say , the number of parameters if model is high means that model is good.

- quetion aries how you can access those knowledges--> by prompting --> so as user you can send a query to the llm (technically "prompt" ) --> llm take prompt and try to answer after searching this prompt each word comparision their parameter;; this how llm works

- most of time this work (sending query to llm , llm take your query and try to answer using their knowledge or parameters) good.

- there are certain situation where this particular flow does not work. (where you can't be able to generate good response from llm's parametric knowledge). eg three situations

  1.(Private data) we ask a question which depends on my person data. (during the pretraining stage of llm it couldn't see those data).

2. (recent data) ask about today's infomation (about those data which llm haven't seen during there pretraining process). # each llm has their last knowledge cutoff date.

## doubt: we can ask these todays data or like information to chat-gpt bz it has access to internet.

3.( helusination): in some question you send you query to llm and try to access parametric knowledge --> your llm give facturly incorrect information with confidence ;; bz llm works with probabilistically ;; that is why there is probability that instead of giving facturally correct information , they assume something itself and give you

- is there any way so we can solve these three problem?, and get good response from llm's parameters knowledge? --> yes --> there is one way to solve these problem (not much effeciently but yes ). --> by doing **Fine-tunning**

## now let try to understant what is **fine-tunning**

- take a pretained llm model --> and train again on a smaller - domain specific dataset

- you have llm (trained on huge datasets) which has knowledge about all the feild but we want that , that llm model should also hava our person data knowledge --> so what we do is

- we qurate a small dataset (which belongs to our domain) , and try to train that llm on it.;; after that it doesn't only have knowledge about more field also knowledge about our datasets. --> and know it can also answer a difficult answer from our field side.

## there are diff types of fine-tunning;;

1. Supervised fine-tunning; where we provied a labeled dataset to our model. eg

- (our prompt --> desired output) -> format of this dataset. (1k - 1M like datasets)

2. Contineud pretraining ;; this a **unsupervised way** to fine-tune our model , here also we provide datasets but that dataset would be unsupervised (labels will be not there)

## there is also some other fine-tunning technic

-> RLHF, LORA, KULORA

## how exactly carry out process of Fine-tunning;; (for supervised fine-tunning) --> presisly 4 steps process

1. Collect data -> A few hundred - few hundred-thousand carefully curated examples (prompt --> desired output)

2. Choose a methods -> Full-parameter FT, LoRA/QLoRA, or parameter-efficient adapters.

3. Train for a few epochs -> You keep the base weights frozen or partially frozen and update only a small subset (LoRA) or all weights (full FT).

4. Evaluate and safety-test -> Measure exact-match, factually, and hallucination rate against held-out data; red-team for safety.

16
