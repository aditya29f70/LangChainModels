behind the seen chains work bz they are made through runnables , you should know what runnable work 
for chains

* you already know runnable come in field because all langchain feature were not in standardise to invoke them 
so we didn't able to connect them (like of invoking 'prompt' we had 'format', for invoke llm we had 'predict' etc ) that why runnable came in field 
make this task easy --> standardise it to ''invoke''

## you have to know type of runnable in langchain -> 2
1. task specific runnable 
-> def: these are core Langchain components that have converted into runnable so they can be used in pipeline
-> purpose -> perform task-specific like llm calls, prompting, retrieval etc.
-> eg: 
ChatOpenAI -- run an llm modl (these are core component as well runnable)
PromptTemplate --> formats prompt dynamically
Retrieval --> Retrieval relavant documents

2. runnable primitives
-> def: these are fundamental building blocks for structuring executing logic in AI workflows
-> purpose: They help orchestrate execution by defining how different Runnable interact
(sequentially, in parallel, conditionally, etc )

* RunnableSequence --> runs steps in order (| opertor).
* RunnableParallel --> runs multiple steps **simultaneously.**
* RunnableMap --> Maps the same input across multiple functions.

* RunnableBranch -> Implements conditional execution (if-else logic)

* Runnablelambda -> Wrap custom python functions into Runnables.

* RunnablePassthrough -> just forward input as output (acts as a placeholder).


## today we will see about each Runnable Primitive

1. RunnableSequence: 
--> RunnableSq is a sequencial chain of runnables in langchain that executes each step one after another, passing the output of one step as the input to the next.

--> it is useful when you need to compose multiple runnable together in a structure workflow.

* we can easly sequencially connect two or more than two runnables --> usally same as '|'
