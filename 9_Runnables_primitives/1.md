behind the seen chains work bz they are made through runnables , you should know what runnable work 
for chains

* you already know runnable come in field because all langchain feature were not in standardise to invoke them 
so we didn't able to connect them (like of invoking 'prompt' we had 'format', for invoke llm we had 'predict' etc ) that why runnable came in field 
make this task easy --> standardise it to ''invoke''

## you have to know type of runnable in langchain -> 2
1. task specific runnable 
-> def: these are core Langchain components that have converted into runnable so they can be used in pipeline
-> purpose -> perform task-specific like llm calls, prompting, retrieval etc.
-> eg: 
ChatOpenAI -- run an llm modl (these are core component as well runnable)
PromptTemplate --> formats prompt dynamically
Retrieval --> Retrieval relavant documents

2. runnable primitives
-> def: these are fundamental building blocks for structuring executing logic in AI workflows
-> purpose: They help orchestrate execution by defining how different Runnable interact
(sequentially, in parallel, conditionally, etc )

* RunnableSequence --> runs steps in order (| opertor).
* RunnableParallel --> runs multiple steps **simultaneously.**
* RunnableMap --> Maps the same input across multiple functions.

* RunnableBranch -> Implements conditional execution (if-else logic)

* Runnablelambda -> Wrap custom python functions into Runnables.

* RunnablePassthrough -> just forward input as output (acts as a placeholder).


## today we will see about each Runnable Primitive

1. RunnableSequence: 
--> RunnableSq is a sequencial chain of runnables in langchain that executes each step one after another, passing the output of one step as the input to the next.

--> it is useful when you need to compose multiple runnable together in a structure workflow.

* we can easly sequencially connect two or more than two runnables --> usally same as '|'

2. RunnableParallel:
RunnableParallel is a runnable that allows multiple runnables to execute in parallel.

Each runnable receieve the same input and processes it independently, producing a *******dictionary of outputs*******.

3. RunnablePassthrough
=> is a special Runnable premitive that simply returns the input as output without modifying it.
take a eg ; what it can be useful

eg. in sequencial runnable where we were asking for a joke and it's explaintion but at the end we god only explaintion (but if we want to know that joke also (so here we can use RunnablePassthrough))

what we can do first we make a joke_gen_chain which will take (promt -> llm -> parser) now try to make parallel chain where one of the its branch would be RunnablePassthrough and other would be other sq chain which will convert output's explaintion


4. RunnableLambda:
--> RunnableLambda is a runnable primitive that allows you to apply custom python functions within an Ai pipeline 

--> it acts as a middleware between different Ai commponents, enabling preprocessing, transformation, **API calls**, filtering, and post-processing in a langChain workflow

eg ; you using it you can do preprocssing works (eg; filtering html tags) --> so this preprocessing works will become a part of this preprocessing steps and automatically will be done

now try to make some -> give a topic to llm and tell to generate joke and now try to calculate number of words in that joke (that will now ask to llm ,we will try to calculate it by ourself, bz llms are bad to do such works)

5. RunnableBranch; used it for conditional chains
RunnableBranch is control flow component in lanchain that allows you to conditionally route input data to different chains or runnable based on custom logic.

--> it functions like an if/elif/else block for chains -- where you define a set of condition functions, each associated with a runnable (eg, LLM call, prompt chain, or tool). The first maching condition is execute. if no condition matches, a default runnable is used (if provided).

eg ; if you have a email system in it you a email come to you and you have to execute diff works according to that email like 
1. if that email is about complain then --> forward to customer support 
2. if email is about refund --> sent to database
3. if email is about general query then try to solve by our chatbot

* email --> prompt (design a prompt to ask what kind of this email is about ) --> llm --> runnable branch-> (do something if it is complain) (do something if general query) ( do something if it is refund related)

* what we will do (take topic from user) --> creat a prompt accordingly to tell llm to generate a report on this topic --> llm --> parser --> Runnablebranch() -> (if words > 500, ask another llm to summarize that report) (if less than 500 so as it print)

* since we are using RunnableBranch so note(it input would be in structure format(validation mush be checked))

## LCEL (langchain expration language)
which runnable you saw which was used in most of the place --> Runnable Sequence ; so they thought , that would be very tepical to use RunnableSequence so they work on this topic ant was trying to make something so we don't need to write ''RunnableSequence'' always after import it, --> '|'(pipe) come in this field  --> this pipe is called LCEL in langchain.