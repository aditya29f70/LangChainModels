## What are retrievers;

-> A retriever is a component in langchain that fetches relevant documents from a data source in a response to use's query.

-> There are multiple types of retrievers

-> all retrievers in langchain are runnables
(means we can use retriever in chain)

## (data source/could be vector store or apis/main thing data are stored there) , now query will come to retriever now retriever will go to data source and take all to relavent documents ; and when those docments are gotten then it will be fetched by retriever

## you can think it as --> query --> (retriever /kind of fn /like a search engine) --> relavent document objects from datasource

## types of retrievers in langchain: (we can divide it into to categories)

1. (in the basis of **Data source**) -> diff retrievers works with diff **data sources**

- - eg
    -> wikipedia retriever (take your query and search at whole wikipedia) -> give you relavent article \

-> vector_store base retriever \
-> Arkiv retriever (go to that research plateform(arkiv) and give you relavent research paper)

2.  (according to its **search strategy**)

- diff retriever use diff machnisme to search relavent document
- - eg
    -> Maximum marginer relavance (MMR)
    -> Multi-query-retriever
    -> contexture-compresion-retriever

## (1) Wikipedia retriever

-> A wikipedia retriever is a retriever that queries the wikipedia api to fetch relevant content for a given query.

- How it works

1. you give it q query (e.g ; 'Albert Eistein')
2. it sends the query to wikipedia'a API
3. It retrieves the **most relevent articles** (internally some algo works which select more relavent parts, not sementic matching --> here keyword matching happening)
4. it returns them as langChain Document objects

## (2) Vector store retriever

-> A vector store retriever in langchain is the most common type of retriever that lets you search and fetch documents from the vector store based on **semantic simimarity** using vector embeddings.

- How it works

1. You store your documents in a vector store (like FAISS, Chroma, Weaviate)
2. Each document is converted into a dense vector using an embedding model
3. When the use enters a query;

- it's also turned into a vector
- The retriever compares the query vector with the stored vectors
- It retrieves the top-k most similar ones

## (3) Maximal Marginal Relevance (MMR)

"How can we pick results that are not only relevent to the query but also different from each other?"

MMR is an information retrieval algorithm designed to reduce redundancy in the retrieved results while maintaining high relevance to the query.

- Why MMR Retriever?
  In regular similarity search, you may get documents that are:

- - All very similar to each other
- - Repeating the some info
- - Lacking diverse perspectives

- MMR Retriever avoids that by:
- - Picking the **most relevant document** first
- - Then picking the next most relevant **and least similar** to already selected docs
- - And so on...

- This helps especially in RAG pipeline where:

- - you want your context window to contain **diverse but still relevant information**

- - Especially useful when documents are semantically overlapping

## (4) Multi-Query Retriever

- Sometimes a single query might not capture all the ways information in phrased in your documents.

- For eg;

- Query; "How can i stay healthy?"

- Could mean:
- - What should i eat?
- - How often should i exercise?
- - How can i manage stress?

- A simple similarity search might **miss documents** that talk about those things but don't use the world "healthy."

1. Takes your original query
2. Uses an LLM (eg. GPT-3.5) to generate multiple semantically different version of that query
3. performs retrieval for each sub-query
4. combines and dedupliates the results

## what happend some time , user sent query may be ambigues so we don't clear its meaning, so quaity of retrieve docs will not be good

--> so Multi-query retrieve just try to remove this embiguity

- what we do (this embiguity query) --> send to (LLM model) --> try to generated multiple queries

- - eg
- How can i stay healthy?

1. "what are the best foods to maintain good health?"

2. "how often should i exercise to stay fit?

3. "what lifestyle habits improve mental and physical wellness?"

4. "how can i boost my immune system naturally?"

5. "What daily routines support long-term health?"

- these queries are less embiguis (but now you have 5 query from one)

- now you send these 5 queries to your retriever(normal retriever let similarity search retriever) now each retriever search relavent docs from data source according to there question. --> now we will get 5 docs from each retriever --> now merage them and remove duplicate docs if any ; and show that numher of docs which use want to see

- that why it called multi-Query retriever

## (5) Contextual compression Retriever

- The Contextual Compression Retriever in LangChain is an advanced retriever that improves retrieval quality by compressing documents after retrieval - keeping only the relevant content based on the user's query.

- Query:

- - "What is photosynthesis?"

- Retrieved Document (by a traditional retriever):

- The grand canyan is a famous natural site. Photosynthesis how plants convert light energy. Many tourists visit every year." --> this is page_content of a document obj (why this happens bz it is not neccessary that you text-splitter split things always relavent so this can happen)

- Problem:
- - The retriever returns the entire paragraph
- - Only one sentence is actually relevant to the query
- - The rest is irrelevant noise that wastes context window and may confuse the LLM

- What Contextual compression Retriever does: (there is two part in this retriever)

1. pick your query and --> sent to a normal retriever (similarity search retriever) and you said (k=2) you want to 2 relavent docs from data source --> now you send these retriever docs and query to a LLM;; and you will say to llm that "based on the query trim the document content or remove the irrevent things from the doc" --> and you will get new documents where only those content are present which are related to the query

## How it works

1. -> **Base Retriever** (eg. FAISS, Chroma) retriever N documents.
2. -> A \*\*Compressor (usually an LLM) is applied to each document.
3. -> The compressor keeps **only the parts relevant to the query**.
4. -> irrelevant content is discarded.

## When to use

- Your Documents are **long and contain mixed infomation**
- You want to reduce context length\*\* for LLMs
- You need to **improve answer accuracy** in RAG pipeline

# If you want to learn more retriever just go --> https://docs.langchain.com/oss/python/integrations/retrievers

- why there is lot of retriever --> making Rag base application
- when you make Rag base application it's performance not good at time, so for improving this rag system, so depend these advance retriever
